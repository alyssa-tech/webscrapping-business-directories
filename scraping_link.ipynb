{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping every link of each lead from punb link and store in link.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting page 1...\n",
      "Extracting page 2...\n",
      "Extracting page 3...\n",
      "Extracting page 4...\n",
      "Extracting page 5...\n",
      "Extracting page 6...\n",
      "Extracting page 7...\n",
      "Extracting page 8...\n",
      "Extracting page 9...\n",
      "Extracting page 10...\n",
      "Extracting page 11...\n",
      "Extracting page 12...\n",
      "Extracting page 13...\n",
      "Extracting page 14...\n",
      "Extracting page 15...\n",
      "Extracting page 16...\n",
      "Extracting page 17...\n",
      "Extracting page 18...\n",
      "Extracting page 19...\n",
      "Extracting page 20...\n",
      "Extracting page 21...\n",
      "Extracting page 22...\n",
      "Extracting page 23...\n",
      "Extracting page 24...\n",
      "Extracting page 25...\n",
      "Extracting page 26...\n",
      "Extracting page 27...\n",
      "Extracting page 28...\n",
      "Extracting page 29...\n",
      "Extracting page 30...\n",
      "Extracting page 31...\n",
      "Extracting page 32...\n",
      "Extracting page 33...\n",
      "Extracting page 34...\n",
      "Extracting page 35...\n",
      "Extracting page 36...\n",
      "Extracting page 37...\n",
      "Extracting page 38...\n",
      "Extracting page 39...\n",
      "Extracting page 40...\n",
      "Extracting page 41...\n",
      "Extracting page 42...\n",
      "Extracting page 43...\n",
      "Extracting page 44...\n",
      "Extracting page 45...\n",
      "Extracting page 46...\n",
      "Extracting page 47...\n",
      "Extracting page 48...\n",
      "Extracting page 49...\n",
      "Extracting page 50...\n",
      "Extracting page 51...\n",
      "Extracting page 52...\n",
      "Extracting page 53...\n",
      "Extracting page 54...\n",
      "Extracting page 55...\n",
      "Extracting page 56...\n",
      "Extracting page 57...\n",
      "Extracting page 58...\n",
      "Extracting page 59...\n",
      "Extracting page 60...\n",
      "Extracting page 61...\n",
      "Extracting page 62...\n",
      "Extracting page 63...\n",
      "Extracting page 64...\n",
      "Extracting page 65...\n",
      "Extracting page 66...\n",
      "Extracting page 67...\n",
      "Extracting page 68...\n",
      "Extracting page 69...\n",
      "Extracting page 70...\n",
      "Extracting page 71...\n",
      "Extracting page 72...\n",
      "Extracting page 73...\n",
      "Extracting page 74...\n",
      "Extracting page 75...\n",
      "Extracting page 76...\n",
      "Extracting page 77...\n",
      "Extracting page 78...\n",
      "Extracting page 79...\n",
      "Extracting page 80...\n",
      "Extracting page 81...\n",
      "Extracting page 82...\n",
      "Extracting page 83...\n",
      "Extracting page 84...\n",
      "Extracting page 85...\n",
      "Extracting page 86...\n",
      "Extracting page 87...\n",
      "Extracting page 88...\n",
      "Extracting page 89...\n",
      "Extracting page 90...\n",
      "Extracting page 91...\n",
      "Extracting page 92...\n",
      "Extracting page 93...\n",
      "Extracting page 94...\n",
      "Extracting page 95...\n",
      "Done web scrapping\n"
     ]
    }
   ],
   "source": [
    "#List of the company [LINK]\n",
    "companies = []\n",
    "links = []\n",
    "\n",
    "def get_link():\n",
    "    for i in range (1,96):#95 pages\n",
    "        url = f\"https://www.punb.com.my/networkdirectories/index.php?route=directory/search&filter_category_id=0&filter_industry_id=&filter_state_id=&page={i}\"\n",
    "        page = requests.get(url)\n",
    "        page = page.content\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        company_elements = soup.find_all('div', class_='item col-sm-3 col-xs-4')\n",
    "\n",
    "        #extract the company info in the main page\n",
    "        for company in company_elements:\n",
    "            name = company.find('h4').text #.replace(' ','')\n",
    "            owner = company.find('h5').text #.replace(' ','')\n",
    "            address = company.find('div', class_= 'fldcomp_add1 hidden-xs').text\n",
    "            city = company.find ('div', class_= 'fldcomp_add2 hidden-xs').text\n",
    "            state = company.find ('div', class_= 'fldcomp_add3 hidden-xs').text\n",
    "            link = company.a['href']\n",
    "\n",
    "            \n",
    "            # print (f\"Company Name: {name.strip()}\")\n",
    "            # print (f\"Ownership: {owner.strip()}\")\n",
    "            # print (f\"Adress: {address}\")\n",
    "            # print (f\"City: {city}\")\n",
    "            # print (f\"State: {state}\")\n",
    "            # print (f\"Link: {link}\")\n",
    "            # print('')\n",
    "\n",
    "            #store it in the list\n",
    "            companies.append([name, owner, address, city, state, link])\n",
    "            links.append(link)\n",
    "        \n",
    "        print (f\"Extracting page {i}...\")\n",
    "\n",
    "    print ('Done web scrapping')\n",
    "\n",
    "get_link()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the list in the csv\n",
    "df = pd.DataFrame(companies, columns= ['Company Name', 'Ownership','Address','City','State','Link'])\n",
    "link_df= pd.DataFrame(links, columns= ['Link'])\n",
    "df.to_csv('output.csv')\n",
    "link_df.to_csv('temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.punb.com.my/networkdirectories/index.php?route=directory/view&fldcomp_code=20540\n"
     ]
    }
   ],
   "source": [
    "url = links[64]\n",
    "page = requests.get(url)\n",
    "page= page.content\n",
    "print(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
